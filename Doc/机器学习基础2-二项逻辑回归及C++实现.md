
##分类问题##
生活中我们会遇到很多分类问题，如一封邮件是否是垃圾邮件等。那么能否建立一种模型来给出垃圾邮件的概率？通过前面[线性回归一文](http://www.coderjie.com/blog/249556dca4c611e7841d00163e0c0e36)我们知道线性回归的结果是一个连续值，并且它的范围是无法限定的，而分类问题需要我们给出一个0.0~1.0之间的概率值，那么有没有什么办法可以将线性回归的结果值映射为一个0.0~1.0之间的概率值呢？有！它就是逻辑函数。

逻辑函数是一种S函数，函数形式为：

$$
p(t)=\frac{1}{1+e^{-t}}
$$

逻辑函数的函数图像为（来源于维基百科）：

![](http://www.coderjie.com/static/img/2017/12/7114810.svg.png)

##二项逻辑回归模型##
利用逻辑函数我们可以构造二项逻辑回归模型（binomial logistic regression model）。二项逻辑回归模型中的二项意为二分类，也就是分类类别 $y\in{0,1}$ 。

设特征数量为 $n$ 即，特征为 $x$ ，特征的参数为 $\theta$ ，我们定义如下：

\begin{align}
\\\
x & =
\begin{bmatrix}
1 \ x\_1 \ \cdots \ x\_n
\end{bmatrix}
\\\
\\\
\theta & =
\begin{bmatrix}
\theta\_0 \\\\
\theta\_1 \\\\
\vdots    \\\\
\theta\_n
\end{bmatrix}
\\\
\\\
x\theta & = \theta\_0 + \theta\_1x\_1 + \theta\_2x\_2 + \cdots + \theta\_nx\_n \\\\
\\\
h\_\theta(x) & = \frac{1}{1+e^{x\theta}}
\end{align}

 $h\_\theta(x)$ 就是我们的概率函数，我们可以假设它为分类类别为1的概率，那么类别1和类别0的概率分别为：

\begin{align}
\\\
P(y=1 \mid x) & =h\_\theta(x)
\\\
\\\
P(y=0 \mid x) & =1-h\_\theta(x)
\\\
\end{align}

综合起来可以写成：
\begin{align}
\\\
P(y \mid x) & =h\_\theta(x)^y(1-h\_\theta(x))^{1-y}
\\\
\end{align}

设训练样本数为 $m$，训练样本集为 $X$ ，训练输出集为 $Y$ ，如下：
\begin{align}
X & =
\begin{bmatrix}
x^{0}  \\\\
x^{1}  \\\\
\cdots \\\\
x^{m-1}
\end{bmatrix}
\\\
\\\
Y & =
\begin{bmatrix}
y^{0}       \\\\
y^{1}       \\\\
\vdots        \\\\
y^{m-1}
\end{bmatrix}
\\\
\end{align}

我们的目标是已知 $X$ 和 $Y$ 的情况下得到最优的 $\theta$。

##似然函数##
哪个 $\theta$ 是最优的？我们需要先定义似然函数：

\begin{align}
\\\
L(\theta) &= \prod\_{i=0}^{m-1} P(y^i \mid x^i)
\\\
\\\
L(\theta) &= \prod\_{i=0}^{m-1}h\_\theta(x^i)^{y^i} (1-h\_\theta(x^i))^{1-y^i}
\\\
\end{align}

我们在似然函数中引入自然对数以方便后续的求导，则：

\begin{align}
\\\
L(\theta) &= \log(\prod\_{i=0}^{m-1}h\_\theta(x^i)^{y^i} (1-h\_\theta(x^i))^{1-y^i})
\\\
L(\theta) &= \sum\_{i=0}^{m-1}\log(h\_\theta(x^i)^{y^i} (1-h\_\theta(x^i))^{1-y^i})
\\\
L(\theta) &= \sum\_{i=0}^{m-1}(y^i\log(h\_\theta(x^i)) + (1-y^i)\log(1-h\_\theta(x^i)))
\\\
\end{align}

很明显似然函数最大值对应的 $\theta$ 就是我们求解的目标，所以问题变为：
$$
\max\_\theta L\_\theta
$$

##梯度上升法##
使用梯度上升法可以帮助我们找到似然函数的最大值，参数 $\theta\_j$的梯度为：

$$
\frac{\partial L(\theta)}{\partial \theta\_j}=\frac{\partial}{\partial \theta\_j} \sum\_{i=0}^{m-1}(y^i\log(h\_\theta(x^i)) + (1-y^i)\log(1-h\_\theta(x^i)))
$$

假设样本数为1，则得到如下：
\begin{align}
\\\
\frac{\partial L(\theta)}{\partial \theta\_j} &= \frac{\partial}{\partial \theta\_j} (y\log(h\_\theta(x)) + (1-y)\log(1-h\_\theta(x)))
\\\
\\\
\frac{\partial L(\theta)}{\partial \theta\_j} &= \frac{y}{h\_\theta(x)} \frac{\partial}{\partial \theta\_j} h\_\theta(x) + \frac{1-y}{1-h\_\theta(x)}\frac{\partial}{\partial \theta\_j}(1-h\_\theta(x))
\\\
\\\
\frac{\partial L(\theta)}{\partial \theta\_j} &= (\frac{y}{h\_\theta(x)} - \frac{1-y}{1-h\_\theta(x)})\frac{\partial}{\partial \theta\_j} h\_\theta(x)
\\\
\end{align}

又因为：

\begin{align}
\\\
\frac{\partial}{\partial \theta\_j} h\_\theta(x) &= \frac{\partial}{\partial \theta\_j} \frac{1}{1+e^{x\theta}}
\\\
\\\
\frac{\partial}{\partial \theta\_j} h\_\theta(x) &=\frac{-1}{(1+e^{x\theta})^2} \frac{\partial}{\partial \theta\_j}e^{x\theta}
\\\
\\\
\frac{\partial}{\partial \theta\_j} h\_\theta(x) &=\frac{-e^{x\theta}}{(1+e^{x\theta})^2} \frac{\partial}{\partial \theta\_j} x\theta
\\\
\\\
\frac{\partial}{\partial \theta\_j} h\_\theta(x) &=-h\_\theta(x)(1-h\_\theta(x)) \frac{\partial}{\partial \theta\_j} x\theta
\\\
\\\
\frac{\partial}{\partial \theta\_j} h\_\theta(x) &=-h\_\theta(x)(1-h\_\theta(x)) x\_j
\\\
\end{align}

所以：

\begin{align}
\\\
\frac{\partial L(\theta)}{\partial \theta\_j} &= (\frac{y}{h\_\theta(x)} - \frac{1-y}{1-h\_\theta(x)})(-h\_\theta(x)(1-h\_\theta(x)) x\_j)
\\\
\\\
\frac{\partial L(\theta)}{\partial \theta\_j} &= -(y(1-h\_\theta(x)) - (1-y)h\_\theta(x))x\_j
\\\
\\\
\frac{\partial L(\theta)}{\partial \theta\_j} &= -(y-h\_\theta(x))x\_j
\\\
\end{align}

多个样本的正确公式为：
\begin{align}
\\\
\frac{\partial L(\theta)}{\partial \theta\_j} &= -\sum\_{i=0}^{m-1} (y^i-h\_\theta(x^i))x\_j^i
\\\
\end{align}

梯度上升法下的 $\theta\_j$ 的更新公式为：($\alpha$ 为学习速度)

\begin{align}
\\\
\theta\_j &:= \theta\_j + \alpha \frac{\partial L(\theta)}{\partial \theta\_j}
\\\
\\\
\theta\_j &:= \theta\_j - \alpha \sum\_{i=0}^{m-1} (y^i-h\_\theta(x^i))x\_j^i
\\\
\end{align}
