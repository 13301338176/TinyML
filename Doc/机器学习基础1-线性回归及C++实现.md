

##回归问题##
给定多个自变量，一个因变量以及代表它们之间关系的一些训练样本，如何来确定它们之间的关系。

##线性模型##
设特征数量为 $n$ 即自变量的个数，自变量为 $x$ ，自变量的参数为 $\theta$ ，我们定义它们的关系如下：

\begin{align}
h\_\theta(x) & = \theta\_0 + \theta\_1x\_1 + \theta\_2x\_2 + \cdots + \theta\_nx\_n \\\\
& = \theta^T x \\\
\\\
\theta & =
\begin{bmatrix}
\theta\_0 \\\\
\theta\_1 \\\\
\vdots    \\\\
\theta\_n
\end{bmatrix}
\\\
\\\
x & =
\begin{bmatrix}
1       \\\\
x\_1    \\\\
\vdots  \\\\
x\_n
\end{bmatrix}
\\\
\end{align}

设训练样本数为 $m$，训练样本集为 $X$ ，训练输出集为 $Y$ 如下：
\begin{align}
X & =
\begin{bmatrix}
x^{(0)} x^{(1)} \cdots x^{(m-1)}
\end{bmatrix}
\\\
\\\
Y & =
\begin{bmatrix}
y^{(0)}       \\\\
y^{(1)}       \\\\
\vdots        \\\\
y^{(m-1)}
\end{bmatrix}
\\\
\end{align}

我们的目标是已知 $X$ 和 $Y$ 的情况下得到最优的 $\theta$。

##损失函数##
哪个 $\theta$ 是最优的？我们需要先定义损失函数：
$$
J(\theta)=\frac{1}{2}\sum\_{i=0}^{m-1}(h\_\theta(x^i)-y^i)^2
$$
很明显损失函数最小值对应的 $\theta$ 就是我们求解的目标，所以问题变为：
$$
\min\_\theta J\_\theta
$$

##梯度下降法##
使用梯度下降法可以帮助我们找到损失函数的最小值，参数 $\theta\_j$的梯度为：

$$
\frac{\partial J(\theta)}{\partial \theta\_j}
$$

$$
\frac{\partial J(\theta)}{\partial \theta\_j}=\frac{\partial}{\partial \theta\_j} \frac{1}{2}\sum\_{i=0}^{m-1}(h\_\theta(x^i)-y^i)^2
$$

假设样本数为1，则得到如下：
$$
\frac{\partial J(\theta)}{\partial \theta\_j}=\frac{\partial}{\partial \theta\_j} \frac{1}{2}(h\_\theta(x)-y)^2
$$

$$
\frac{\partial J(\theta)}{\partial \theta\_j}=(h\_\theta(x)-y)\frac{\partial}{\partial \theta\_j}(h\_\theta(x)-y)
$$

$$
\frac{\partial J(\theta)}{\partial \theta\_j}=(h\_\theta(x)-y)x\_j
$$
