
##什么是增强学习##

机器学习算法大致可以分为三种：

1. 监督学习(回归，分类)

2. 非监督学习(聚类，降维)

3. 增强学习


增强学习（reinforcement learning, RL）又叫强化学习，增强学习关注的是智能体如何在环境中采取一系列行为，从而获得最大的累积回报。通过增强学习，一个智能体应该知道在什么状态下应该采取什么行动。RL是从环境状态到动作的映射的学习，我们把这个映射称为策略。

增强学习和监督学习的区别主要有以下两点：

1. 增强学习是试错学习（trail-and-error），由于没有直接的指导信息，智能体要不断与环境进行交互，通过试错的方式来获得最佳策略。

2. 延迟回报，增强学习的指导信息很少，而且往往是在事后（最后一个状态）才给出，这就导致了一个问题，就是获得正回报或者负回报以后，如何将回报分配给前面的状态。

##马尔可夫决策过程##
智能体在环境中的一系列行为可以用马尔可夫决策过程来表示（markov decision process, MDP）。在说马尔可夫决策过程前先说一下，马尔可夫性（无后效性），也就是指系统的下个状态只与当前状态信息有关，而与更早之前的状态无关。马尔可夫决策过程（markov decision process, MDP）也具有马尔可夫性，MDP不仅考虑状态还考虑动作，即系统下个状态不仅和当前状态有关，也和当前采取的动作有关。例如下棋，当我们在某个局面（状态 $s$ ）走了一步（动作 $a$ ），这时对手的选择（导致下个状态 $s'$ ）我们是不能确定的，但是他的选择只和 $s$ 和 $a$ 有关，而不用考虑更早之前的状态和动作，即 $s'$ 是根据 $s$ 和 $a$ 随机生成的。

一个马尔可夫决策过程由一个四元组构成 $M=(S, A, P\_{sa}, R)$ 

- $S$: 表示状态集合(states)，有 $s \in S$ ， $s\_i$表示第i步的状态。
- $A$: 表示动作集合(actions)，有 $a \in A$ ， $a\_i$表示第i步的动作。
- $P\_{sa}$: 表示状态转移概率集合，$P\_{sa}$ 表示实在当前 $s \in S$ 状态下，经过 $a \in A$作用后，会转移到其他状态的概率分布情况，比如在状态 $s$ 下执行动作 $a$ ，转移到 $s'$ 的概率可以表示为 $p(s'|s, a)$
- $R: S \times A \to \mathbb{R}$，$R$ 是回报函数（reward function），是状态集合 $S$ 和动作集合 $A$ 的积到实数 $\mathbb{R}$ 的映射，表示该状态下该动作的立即回报值，记为 $r(s, a)$ 。

MDP的动态过程如下：某个智能体（agent）的初始化状态为 $s\_0$ ，然后从 $A$ 中挑选一个动作 $a\_0$ 执行。执行后，agent按 $P\_{sa}$概率随机转移到了下一个 $s\_1$状态。然后再执行一个动作 $a\_1$ ，就转移到了 $s\_2$ ，接下来再执行 $a\_2$ ...，如下图：

![](http://www.coderjie.com/static/img/2018/8/814436.png)

##折算累计回报值函数##
增强学习学到的是一个从环境状态到动作的映射（即行为策略），记为策略 $\Pi: S \to A$ 。而增强学习往往又具有延迟回报的特点：如果再第 $n$ 步输掉了棋，那么只有状态 $s\_n$ 和动作 $a\_n$ 获得了立即回报 $r(s\_n, a\_n)=-1$ ，前面的所有状态立即回报均为0。所以对于之前的任意状态 $s$ 和动作 $a$ ，立即回报函数 $r(s, a)$ 无法说明决策的好坏。因而需要定义值函数（value function）来表明当前状态下策略 $\pi$的长期影响。

当我们遵循某个策略 $\pi$ ，所以有 $a\_0=\pi(s\_0), a\_1=\pi(s\_1), a\_2=\pi(s\_2) \cdots$ ，我们将值函数定义如下：
\begin{align}
V^\pi(s) & =E[r(s\_0, a\_0)) + \gamma r(s\_1, a\_1) + \gamma^2 r(s\_2, a\_2) + \cdots | s\_0=s, \pi] \\\\
& = E[r(s\_0, a\_0) + \gamma E[r(s\_1, a\_1) + \gamma r(s\_2, a\_2) + \cdots ] | s\_0=s, \pi] \\\\
& = E[r(s, \pi(s)) + \gamma V^\pi(s')] \\\\
\end{align}

上述式中 $\gamma \in [0, 1]$ 称为折合因子，表明了未来的回报相对于当前回报的重要程度。特别的， $\gamma=0$ 时，相当于只考虑立即回报不考虑长期回报，$\gamma=1$ 时，将长期回报和立即回报看得同等重要。

因为下个时刻将以概率 $p(s'|s, \pi(s))$ 转向下个状态 $s'$ ，所以上式子的期望可以写为：
\begin{align}
V^\pi(s) &= E[r(s, \pi(s)) + \gamma V^\pi(s')] \\\\
&= \sum\_{s' \in S}p(s'|s, \pi(s))[r(s, \pi(s)) + \gamma V^\pi(s')] \\\\
&= r(s, \pi(s)) + \gamma \sum\_{s' \in S}p(s'|s, \pi(s))V^\pi(s') \\\\
\end{align}

值函数表明的是当前状态下策略 $\pi$ 的长期影响，也就是说策略 $\pi$ 越好，值函数越大。

我们的目的就是要找一个在任意初始条件 $s$ 下都能最大化值函数的策略，所以MDP的最优策略如下式：

$$\pi^*= arg \\ \max\_\pi V^\pi(s), (\forall s)$$

##动作值函数##
上面我们的值函数只与状态 $s$ 有关，我们定义一个和状态，动作都有关的动作值函数，如下：
\begin{align}
Q^\pi (s, a) &= E[r(s\_0, a\_0) + \gamma r(s\_1, a\_1) + \gamma^2 r(s\_2, a\_2) + \cdots | s\_0=s, a\_0 = a, \pi] \\\\
&= E[r(s, a) + \gamma Q^\pi(s', \pi(s'))] \\\\
&= \sum\_{s' \in S}p(s'|s, a)[r(s, a) + \gamma Q^\pi(s', \pi(s'))] \\\\
&= r(s, a) + \gamma \sum\_{s' \in S}p(s'|s, a)Q^\pi(s', \pi(s')) \\\\
\end{align}

很显然 $Q^\pi(s', \pi(s'))$ 即为 $V^\pi(s')$ ，所以有：
$$Q^\pi (s, a)=r(s, a) + \gamma \sum\_{s' \in S}p(s'|s, a)V^\pi(s')$$

动作值函数表明的是当前状态和动作下策略 $\pi$ 的长期影响。那么在指定策略和状态的条件下，不同动作对应的动作值就是代表该动作的价值。

##贝尔曼方程##
根据值函数和动作值函数我们得到了两个方程：
\begin{align}
V^\pi(s) &= r(s, \pi(s)) + \gamma \sum\_{s' \in S}p(s'|s, \pi(s))V^\pi(s') \\\\
Q^\pi (s, a) &= r(s, a) + \gamma \sum\_{s' \in S}p(s'|s, a)V^\pi(s') \\\\
\end{align}

记最优策略 $ \pi^* $ ，那么 $ \pi^* $ 对应的状态值函数 $V^* (s)$ 和动作值函数 $Q^* (s, a)$ 都为最大化  ，
很容易理解最优的策略 $ \pi^* $ ，就是选择动作值最大的去执行，所以有如下关系：

$$
V^* (s) = \max\_a Q^* (s,a)
$$

所以得到如下的最优化方程：

$$
Q^* (s, a) = r(s, a) + \gamma \sum\_{s' \in S}p(s'|s, a)\max\_{a'} Q^* (s',a')
$$

#Q-Table###

根据上面公式，只要能我们计算出所有的动作值，也就找到了最优策略 $ \pi^* $。动作值可以用矩阵表示(Q-Table)：
$$
\begin{array} {c|ccc}
\\ & a\_0 & a\_1 & a\_2 \\\\
\hline
s\_0 & Q(0, 0) & Q(0, 1) & Q(0, 2) \\\\
s\_1 & Q(1, 0) & Q(1, 1) & Q(1, 2) \\\\
s\_2 & Q(2, 0) & Q(2, 1) & Q(2, 2) \\\\
\end{array}
$$

 